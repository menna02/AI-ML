{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9174991,"sourceType":"datasetVersion","datasetId":5544918}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mennatullaheisawy/english-french-translation-using-keras?scriptVersionId=192839805\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport keras\nimport os\nfrom pathlib import Path","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-16T07:24:14.77191Z","iopub.execute_input":"2024-08-16T07:24:14.772675Z","iopub.status.idle":"2024-08-16T07:24:27.690922Z","shell.execute_reply.started":"2024-08-16T07:24:14.77264Z","shell.execute_reply":"2024-08-16T07:24:27.689885Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-16 07:24:16.736256: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-16 07:24:16.736351: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-16 07:24:16.876541: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 64  # Batch size for training.\nepochs = 100  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nnum_samples = 10000  # Number of samples to train on.\n# Path to the data txt file on disk.\ndata_path = os.path.join('/kaggle/input/english-france-dictionary', \"fra.txt\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T07:31:15.232427Z","iopub.execute_input":"2024-08-16T07:31:15.232822Z","iopub.status.idle":"2024-08-16T07:31:15.23772Z","shell.execute_reply.started":"2024-08-16T07:31:15.232793Z","shell.execute_reply":"2024-08-16T07:31:15.236759Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Vectorize the data.\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\nwith open(data_path, \"r\", encoding=\"utf-8\") as f:\n    lines = f.read().split(\"\\n\")\n    \n\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text, _ = line.split(\"\\t\")\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_text = \"\\t\" + target_text + \"\\n\"\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T07:36:37.171432Z","iopub.execute_input":"2024-08-16T07:36:37.171833Z","iopub.status.idle":"2024-08-16T07:36:37.397215Z","shell.execute_reply.started":"2024-08-16T07:36:37.1718Z","shell.execute_reply":"2024-08-16T07:36:37.396416Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"input_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint(\"Number of samples:\", len(input_texts))\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-16T07:37:31.914363Z","iopub.execute_input":"2024-08-16T07:37:31.915011Z","iopub.status.idle":"2024-08-16T07:37:31.923882Z","shell.execute_reply.started":"2024-08-16T07:37:31.914978Z","shell.execute_reply":"2024-08-16T07:37:31.923011Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Number of samples: 10000\nNumber of unique input tokens: 70\nNumber of unique output tokens: 91\nMax sequence length for inputs: 14\nMax sequence length for outputs: 59\n","output_type":"stream"}]},{"cell_type":"code","source":"input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n\nencoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n    dtype=\"float32\",\n)\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype=\"float32\",\n)\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype=\"float32\",\n)\n\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1.0\n        \n    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t, target_token_index[char]] = 1.0\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0","metadata":{"execution":{"iopub.status.busy":"2024-08-16T07:59:18.649946Z","iopub.execute_input":"2024-08-16T07:59:18.650326Z","iopub.status.idle":"2024-08-16T07:59:19.265055Z","shell.execute_reply.started":"2024-08-16T07:59:18.650296Z","shell.execute_reply":"2024-08-16T07:59:19.26404Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Define an input sequence and process it.\nencoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\nencoder = keras.layers.LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\ndecoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T08:05:05.384773Z","iopub.execute_input":"2024-08-16T08:05:05.385611Z","iopub.status.idle":"2024-08-16T08:05:06.576147Z","shell.execute_reply.started":"2024-08-16T08:05:05.385575Z","shell.execute_reply":"2024-08-16T08:05:06.575195Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n)\nmodel.fit(\n    [encoder_input_data, decoder_input_data],\n    decoder_target_data,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_split=0.2,\n)\n# Save model\nmodel.save(\"s2s_model.keras\")","metadata":{"execution":{"iopub.status.busy":"2024-08-16T08:05:35.319615Z","iopub.execute_input":"2024-08-16T08:05:35.320616Z","iopub.status.idle":"2024-08-16T08:08:10.53122Z","shell.execute_reply.started":"2024-08-16T08:05:35.320565Z","shell.execute_reply":"2024-08-16T08:08:10.530416Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - accuracy: 0.7058 - loss: 1.5543 - val_accuracy: 0.7084 - val_loss: 1.0750\nEpoch 2/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7464 - loss: 0.9594 - val_accuracy: 0.7256 - val_loss: 0.9697\nEpoch 3/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7610 - loss: 0.8655 - val_accuracy: 0.7478 - val_loss: 0.8850\nEpoch 4/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7851 - loss: 0.7683 - val_accuracy: 0.7691 - val_loss: 0.7800\nEpoch 5/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8023 - loss: 0.6936 - val_accuracy: 0.7877 - val_loss: 0.7246\nEpoch 6/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8146 - loss: 0.6379 - val_accuracy: 0.8017 - val_loss: 0.6929\nEpoch 7/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8239 - loss: 0.6028 - val_accuracy: 0.8067 - val_loss: 0.6613\nEpoch 8/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8282 - loss: 0.5848 - val_accuracy: 0.8146 - val_loss: 0.6404\nEpoch 9/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8344 - loss: 0.5621 - val_accuracy: 0.8202 - val_loss: 0.6183\nEpoch 10/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8402 - loss: 0.5427 - val_accuracy: 0.8215 - val_loss: 0.6072\nEpoch 11/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8448 - loss: 0.5284 - val_accuracy: 0.8286 - val_loss: 0.5906\nEpoch 12/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8498 - loss: 0.5124 - val_accuracy: 0.8283 - val_loss: 0.5877\nEpoch 13/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8541 - loss: 0.4971 - val_accuracy: 0.8370 - val_loss: 0.5631\nEpoch 14/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8574 - loss: 0.4851 - val_accuracy: 0.8382 - val_loss: 0.5615\nEpoch 15/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8611 - loss: 0.4762 - val_accuracy: 0.8419 - val_loss: 0.5428\nEpoch 16/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8639 - loss: 0.4642 - val_accuracy: 0.8443 - val_loss: 0.5328\nEpoch 17/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8674 - loss: 0.4513 - val_accuracy: 0.8458 - val_loss: 0.5264\nEpoch 18/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8704 - loss: 0.4402 - val_accuracy: 0.8488 - val_loss: 0.5157\nEpoch 19/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8727 - loss: 0.4329 - val_accuracy: 0.8498 - val_loss: 0.5129\nEpoch 20/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8748 - loss: 0.4249 - val_accuracy: 0.8531 - val_loss: 0.5028\nEpoch 21/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.8763 - loss: 0.4170 - val_accuracy: 0.8544 - val_loss: 0.4967\nEpoch 22/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8790 - loss: 0.4064 - val_accuracy: 0.8552 - val_loss: 0.4939\nEpoch 23/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8800 - loss: 0.4037 - val_accuracy: 0.8570 - val_loss: 0.4889\nEpoch 24/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8820 - loss: 0.3961 - val_accuracy: 0.8576 - val_loss: 0.4847\nEpoch 25/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8847 - loss: 0.3892 - val_accuracy: 0.8589 - val_loss: 0.4828\nEpoch 26/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8863 - loss: 0.3823 - val_accuracy: 0.8600 - val_loss: 0.4779\nEpoch 27/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8883 - loss: 0.3755 - val_accuracy: 0.8609 - val_loss: 0.4732\nEpoch 28/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8910 - loss: 0.3676 - val_accuracy: 0.8618 - val_loss: 0.4692\nEpoch 29/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8918 - loss: 0.3628 - val_accuracy: 0.8622 - val_loss: 0.4683\nEpoch 30/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8928 - loss: 0.3596 - val_accuracy: 0.8615 - val_loss: 0.4669\nEpoch 31/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8950 - loss: 0.3517 - val_accuracy: 0.8638 - val_loss: 0.4622\nEpoch 32/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8972 - loss: 0.3438 - val_accuracy: 0.8629 - val_loss: 0.4622\nEpoch 33/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8972 - loss: 0.3435 - val_accuracy: 0.8657 - val_loss: 0.4578\nEpoch 34/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9000 - loss: 0.3362 - val_accuracy: 0.8650 - val_loss: 0.4574\nEpoch 35/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9007 - loss: 0.3325 - val_accuracy: 0.8676 - val_loss: 0.4503\nEpoch 36/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9016 - loss: 0.3280 - val_accuracy: 0.8669 - val_loss: 0.4530\nEpoch 37/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9038 - loss: 0.3236 - val_accuracy: 0.8674 - val_loss: 0.4512\nEpoch 38/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9040 - loss: 0.3203 - val_accuracy: 0.8687 - val_loss: 0.4492\nEpoch 39/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9065 - loss: 0.3136 - val_accuracy: 0.8689 - val_loss: 0.4495\nEpoch 40/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9075 - loss: 0.3093 - val_accuracy: 0.8681 - val_loss: 0.4494\nEpoch 41/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9088 - loss: 0.3062 - val_accuracy: 0.8709 - val_loss: 0.4464\nEpoch 42/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9106 - loss: 0.2999 - val_accuracy: 0.8714 - val_loss: 0.4442\nEpoch 43/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9107 - loss: 0.2987 - val_accuracy: 0.8706 - val_loss: 0.4457\nEpoch 44/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9124 - loss: 0.2932 - val_accuracy: 0.8717 - val_loss: 0.4429\nEpoch 45/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9151 - loss: 0.2852 - val_accuracy: 0.8720 - val_loss: 0.4434\nEpoch 46/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9150 - loss: 0.2843 - val_accuracy: 0.8721 - val_loss: 0.4453\nEpoch 47/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9163 - loss: 0.2790 - val_accuracy: 0.8738 - val_loss: 0.4414\nEpoch 48/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9182 - loss: 0.2744 - val_accuracy: 0.8724 - val_loss: 0.4455\nEpoch 49/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9183 - loss: 0.2734 - val_accuracy: 0.8726 - val_loss: 0.4432\nEpoch 50/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9195 - loss: 0.2679 - val_accuracy: 0.8735 - val_loss: 0.4478\nEpoch 51/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9216 - loss: 0.2619 - val_accuracy: 0.8747 - val_loss: 0.4397\nEpoch 52/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9225 - loss: 0.2602 - val_accuracy: 0.8741 - val_loss: 0.4434\nEpoch 53/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9238 - loss: 0.2546 - val_accuracy: 0.8741 - val_loss: 0.4455\nEpoch 54/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9246 - loss: 0.2520 - val_accuracy: 0.8752 - val_loss: 0.4434\nEpoch 55/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9258 - loss: 0.2484 - val_accuracy: 0.8745 - val_loss: 0.4442\nEpoch 56/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9273 - loss: 0.2441 - val_accuracy: 0.8740 - val_loss: 0.4486\nEpoch 57/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9268 - loss: 0.2427 - val_accuracy: 0.8741 - val_loss: 0.4496\nEpoch 58/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9290 - loss: 0.2373 - val_accuracy: 0.8753 - val_loss: 0.4460\nEpoch 59/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9289 - loss: 0.2373 - val_accuracy: 0.8748 - val_loss: 0.4463\nEpoch 60/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9309 - loss: 0.2314 - val_accuracy: 0.8752 - val_loss: 0.4503\nEpoch 61/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9320 - loss: 0.2263 - val_accuracy: 0.8756 - val_loss: 0.4513\nEpoch 62/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9329 - loss: 0.2232 - val_accuracy: 0.8759 - val_loss: 0.4526\nEpoch 63/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9337 - loss: 0.2227 - val_accuracy: 0.8755 - val_loss: 0.4512\nEpoch 64/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9349 - loss: 0.2187 - val_accuracy: 0.8759 - val_loss: 0.4524\nEpoch 65/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9362 - loss: 0.2142 - val_accuracy: 0.8746 - val_loss: 0.4571\nEpoch 66/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9366 - loss: 0.2115 - val_accuracy: 0.8758 - val_loss: 0.4566\nEpoch 67/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9377 - loss: 0.2069 - val_accuracy: 0.8763 - val_loss: 0.4569\nEpoch 68/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9385 - loss: 0.2045 - val_accuracy: 0.8766 - val_loss: 0.4600\nEpoch 69/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9395 - loss: 0.2026 - val_accuracy: 0.8765 - val_loss: 0.4590\nEpoch 70/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9399 - loss: 0.2002 - val_accuracy: 0.8763 - val_loss: 0.4624\nEpoch 71/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9408 - loss: 0.1976 - val_accuracy: 0.8763 - val_loss: 0.4636\nEpoch 72/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9428 - loss: 0.1932 - val_accuracy: 0.8765 - val_loss: 0.4669\nEpoch 73/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9421 - loss: 0.1921 - val_accuracy: 0.8756 - val_loss: 0.4687\nEpoch 74/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9435 - loss: 0.1879 - val_accuracy: 0.8763 - val_loss: 0.4698\nEpoch 75/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9442 - loss: 0.1874 - val_accuracy: 0.8766 - val_loss: 0.4681\nEpoch 76/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9449 - loss: 0.1820 - val_accuracy: 0.8756 - val_loss: 0.4745\nEpoch 77/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9458 - loss: 0.1799 - val_accuracy: 0.8757 - val_loss: 0.4766\nEpoch 78/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9464 - loss: 0.1793 - val_accuracy: 0.8767 - val_loss: 0.4771\nEpoch 79/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9471 - loss: 0.1764 - val_accuracy: 0.8755 - val_loss: 0.4815\nEpoch 80/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9487 - loss: 0.1709 - val_accuracy: 0.8768 - val_loss: 0.4826\nEpoch 81/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9488 - loss: 0.1700 - val_accuracy: 0.8754 - val_loss: 0.4896\nEpoch 82/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9482 - loss: 0.1712 - val_accuracy: 0.8763 - val_loss: 0.4869\nEpoch 83/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9501 - loss: 0.1647 - val_accuracy: 0.8757 - val_loss: 0.4898\nEpoch 84/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9518 - loss: 0.1611 - val_accuracy: 0.8757 - val_loss: 0.4921\nEpoch 85/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9514 - loss: 0.1619 - val_accuracy: 0.8755 - val_loss: 0.4936\nEpoch 86/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9524 - loss: 0.1595 - val_accuracy: 0.8761 - val_loss: 0.4930\nEpoch 87/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9531 - loss: 0.1561 - val_accuracy: 0.8764 - val_loss: 0.4973\nEpoch 88/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9533 - loss: 0.1547 - val_accuracy: 0.8755 - val_loss: 0.5003\nEpoch 89/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9545 - loss: 0.1508 - val_accuracy: 0.8756 - val_loss: 0.5014\nEpoch 90/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9549 - loss: 0.1493 - val_accuracy: 0.8763 - val_loss: 0.5060\nEpoch 91/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9558 - loss: 0.1464 - val_accuracy: 0.8757 - val_loss: 0.5084\nEpoch 92/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9563 - loss: 0.1457 - val_accuracy: 0.8749 - val_loss: 0.5090\nEpoch 93/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9568 - loss: 0.1434 - val_accuracy: 0.8762 - val_loss: 0.5126\nEpoch 94/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9575 - loss: 0.1423 - val_accuracy: 0.8747 - val_loss: 0.5150\nEpoch 95/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9583 - loss: 0.1400 - val_accuracy: 0.8753 - val_loss: 0.5196\nEpoch 96/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9580 - loss: 0.1385 - val_accuracy: 0.8750 - val_loss: 0.5212\nEpoch 97/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9587 - loss: 0.1367 - val_accuracy: 0.8760 - val_loss: 0.5217\nEpoch 98/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9592 - loss: 0.1348 - val_accuracy: 0.8745 - val_loss: 0.5309\nEpoch 99/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9599 - loss: 0.1323 - val_accuracy: 0.8759 - val_loss: 0.5247\nEpoch 100/100\n\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9606 - loss: 0.1296 - val_accuracy: 0.8750 - val_loss: 0.5317\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define sampling models\n# Restore the model and construct the encoder and decoder.\nmodel = keras.models.load_model(\"s2s_model.keras\")\n\nencoder_inputs = model.input[0]  # input_1\nencoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\nencoder_states = [state_h_enc, state_c_enc]\nencoder_model = keras.Model(encoder_inputs, encoder_states)\n\ndecoder_inputs = model.input[1]  # input_2\ndecoder_state_input_h = keras.Input(shape=(latent_dim,))\ndecoder_state_input_c = keras.Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_lstm = model.layers[3]\ndecoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs\n)\ndecoder_states = [state_h_dec, state_c_dec]\ndecoder_dense = model.layers[4]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = keras.Model(\n    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n)\n\n# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq, verbose=0)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = \"\"\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value, verbose=0\n        )\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.0\n\n        # Update states\n        states_value = [h, c]\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2024-08-16T08:16:55.641604Z","iopub.execute_input":"2024-08-16T08:16:55.642262Z","iopub.status.idle":"2024-08-16T08:16:55.790272Z","shell.execute_reply.started":"2024-08-16T08:16:55.642227Z","shell.execute_reply":"2024-08-16T08:16:55.789534Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"for seq_index in range(20):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index : seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print(\"-\")\n    print(\"Input sentence:\", input_texts[seq_index])\n    print(\"Decoded sentence:\", decoded_sentence)","metadata":{"execution":{"iopub.status.busy":"2024-08-16T08:17:06.918653Z","iopub.execute_input":"2024-08-16T08:17:06.919053Z","iopub.status.idle":"2024-08-16T08:17:17.282467Z","shell.execute_reply.started":"2024-08-16T08:17:06.919025Z","shell.execute_reply":"2024-08-16T08:17:17.281489Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"-\nInput sentence: Go.\nDecoded sentence: Va !\n\n-\nInput sentence: Go.\nDecoded sentence: Va !\n\n-\nInput sentence: Go.\nDecoded sentence: Va !\n\n-\nInput sentence: Go.\nDecoded sentence: Va !\n\n-\nInput sentence: Hi.\nDecoded sentence: Salut.\n\n-\nInput sentence: Hi.\nDecoded sentence: Salut.\n\n-\nInput sentence: Run!\nDecoded sentence: Filez !\n\n-\nInput sentence: Run!\nDecoded sentence: Filez !\n\n-\nInput sentence: Run!\nDecoded sentence: Filez !\n\n-\nInput sentence: Run!\nDecoded sentence: Filez !\n\n-\nInput sentence: Run!\nDecoded sentence: Filez !\n\n-\nInput sentence: Run!\nDecoded sentence: Filez !\n\n-\nInput sentence: Run!\nDecoded sentence: Filez !\n\n-\nInput sentence: Run!\nDecoded sentence: Filez !\n\n-\nInput sentence: Run.\nDecoded sentence: Filez !\n\n-\nInput sentence: Run.\nDecoded sentence: Filez !\n\n-\nInput sentence: Run.\nDecoded sentence: Filez !\n\n-\nInput sentence: Run.\nDecoded sentence: Filez !\n\n-\nInput sentence: Run.\nDecoded sentence: Filez !\n\n-\nInput sentence: Run.\nDecoded sentence: Filez !\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
